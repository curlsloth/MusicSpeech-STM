{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "886e5242-824a-4224-a27b-0c22ef686f0b",
   "metadata": {},
   "source": [
    "python3 -m pip install -U git+https://github.com/adefossez/demucs#egg=demucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5da2fa71-e9ea-4172-b7b4-1a4dcad6ee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import demucs.api\n",
    "import soundfile as sf\n",
    "import torchaudio\n",
    "import torch\n",
    "import scipy.io as sio\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "from scipy.io import wavfile\n",
    "\n",
    "# import kagglehub\n",
    "# import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf948c89-ac31-436b-b7a0-d7cedaf7df6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to model files: yamnet/tensorFlow2/yamnet/1\n"
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "# path = kagglehub.model_download(\"google/yamnet/tensorFlow2/yamnet\")\n",
    "path = 'yamnet/tensorFlow2/yamnet/1'\n",
    "\n",
    "print(\"Path to model files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01cb22c0-b83e-4146-9d00-9f321103e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.saved_model.load(\n",
    "    path, tags=None, options=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed5b2005-6670-40d3-b9d2-0a0895ded44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize with default parameters:\n",
    "separator = demucs.api.Separator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f611c2c2-26c0-4435-9a17-488c16a9ebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import librosa\n",
    "# y, sr = librosa.load('/Users/andrewchang/NYU_research/MusicSpeech-STM/data/musicCorp/fma_large/000/000002.mp3')\n",
    "# y, sr = sf.read('/Users/andrewchang/NYU_research/MusicSpeech-STM/data/musicCorp/fma_large/000/000002.mp3')\n",
    "y, sr = torchaudio.load('/Users/andrewchang/NYU_research/MusicSpeech-STM/data/musicCorp/fma_large/000/000002.mp3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd947fcb-c990-4974-8d76-da0ce7c0b26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1321344])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19bfb3c7-6b6b-4869-bf8d-ffc5e736066b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acd3eee3-90cf-49be-aa0c-843eac2d913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tensor = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b041b858-0d63-4a39-b39e-1729e066443e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1321344])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# # y_2 = np.stack((y, y))\n",
    "# y_tensor = torch.tensor(y)\n",
    "\n",
    "# if y_tensor.shape[0] > y_tensor.shape[1]:\n",
    "#     y_tensor = y_tensor.T\n",
    "\n",
    "# Duplicate and stack the tensor\n",
    "if y_tensor.shape[0]==1:\n",
    "    y_tensor = torch.stack((y_tensor, y_tensor), dim=0)\n",
    "\n",
    "y_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efb479f0-8a79-4617-947f-414a45234dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function by default will make sure the sampling rate is at 44.1 kHz\n",
    "origin, separated = separator.separate_tensor(wav=y_tensor, sr=sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee1786aa-318b-48bb-a732-117b34cd3b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'drums': tensor([[0.0003, 0.0003, 0.0003,  ..., 0.0013, 0.0007, 0.0009],\n",
       "         [0.0004, 0.0003, 0.0003,  ..., 0.0017, 0.0008, 0.0019]]),\n",
       " 'bass': tensor([[0.0004, 0.0004, 0.0004,  ..., 0.0184, 0.0181, 0.0180],\n",
       "         [0.0004, 0.0004, 0.0005,  ..., 0.0211, 0.0207, 0.0203]]),\n",
       " 'other': tensor([[-2.1604e-04, -2.4037e-05, -5.9582e-05,  ..., -6.5464e-03,\n",
       "          -1.1816e-02, -1.4527e-02],\n",
       "         [ 3.3266e-05,  2.1111e-04,  2.2579e-04,  ..., -1.0578e-03,\n",
       "          -1.1904e-02, -1.3923e-02]]),\n",
       " 'vocals': tensor([[0.0004, 0.0004, 0.0004,  ..., 0.0921, 0.0774, 0.0425],\n",
       "         [0.0003, 0.0003, 0.0004,  ..., 0.1129, 0.1449, 0.1349]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da59f40e-1689-4bcd-94f8-759c68ef11c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1321344])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "separated['vocals'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a717fdc-3080-40e6-846f-c3950483b560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0003, 0.0003, 0.0004,  ..., 0.1025, 0.1112, 0.0887])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "separated['vocals'].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81a4d72d-b0f3-45d2-b662-99c6387ed43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the name of the class with the top score when mean-aggregated across frames.\n",
    "def class_names_from_csv(class_map_csv_text):\n",
    "  \"\"\"Returns list of class names corresponding to score vector.\"\"\"\n",
    "  class_names = []\n",
    "  with tf.io.gfile.GFile(class_map_csv_text) as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "      class_names.append(row['display_name'])\n",
    "\n",
    "  return class_names\n",
    "\n",
    "class_map_path = model.class_map_path().numpy()\n",
    "class_names = class_names_from_csv(class_map_path)\n",
    "\n",
    "# verify and convert a loaded audio is on the proper sample_rate (16K), otherwise it would affect the model's results.\n",
    "\n",
    "def ensure_sample_rate(original_sample_rate, waveform,\n",
    "                       desired_sample_rate=16000):\n",
    "  \"\"\"Resample waveform if required.\"\"\"\n",
    "  if original_sample_rate != desired_sample_rate:\n",
    "    desired_length = int(round(float(len(waveform)) /\n",
    "                               original_sample_rate * desired_sample_rate))\n",
    "    waveform = scipy.signal.resample(waveform, desired_length)\n",
    "  return desired_sample_rate, waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a16b8323-fc0d-4a1d-9b87-ddbd47bbf1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, waveform = ensure_sample_rate(sr, np.array(separated['vocals'].mean(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37e3a2f1-b08c-4372-96d0-c59b3d4cf944",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, embeddings, spectrogram = model(np.array(separated['vocals'].mean(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6120fc22-c727-4e5a-9a52-41a7e0e7b4bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(172, 521), dtype=float32, numpy=\n",
       "array([[5.0781357e-01, 1.9586347e-03, 4.9304729e-04, ..., 1.3524513e-03,\n",
       "        9.8279375e-04, 1.2134526e-08],\n",
       "       [1.8021800e-01, 7.3524085e-03, 1.8393683e-03, ..., 2.4640195e-03,\n",
       "        6.5983087e-03, 3.2770197e-07],\n",
       "       [4.2811926e-02, 7.6749391e-04, 8.6811907e-04, ..., 1.5474551e-03,\n",
       "        2.0208836e-03, 4.6398277e-08],\n",
       "       ...,\n",
       "       [6.3549966e-02, 8.9087925e-04, 8.3256915e-04, ..., 3.7630391e-03,\n",
       "        2.7444386e-03, 3.5474150e-06],\n",
       "       [4.5188186e-03, 1.2748324e-03, 3.7926813e-05, ..., 4.9572048e-04,\n",
       "        3.5652544e-04, 1.5283793e-07],\n",
       "       [7.8315310e-02, 6.5313084e-03, 2.6652061e-03, ..., 1.6982110e-02,\n",
       "        1.1271269e-02, 1.7485968e-04]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19d10434-2a97-4fe5-83a3-8cf49d9ad6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main sound is: Speech\n"
     ]
    }
   ],
   "source": [
    "scores_np = scores.numpy()\n",
    "spectrogram_np = spectrogram.numpy()\n",
    "infered_class = class_names[scores_np.mean(axis=0).argmax()]\n",
    "print(f'The main sound is: {infered_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59b3e012-9b6a-4fcc-8e9b-dbbe7911e72f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.5364147e-01, 4.5595123e-03, 1.1929839e-03, ..., 2.7698800e-03,\n",
       "        1.2314862e-03, 9.1753968e-08],\n",
       "       [1.6486099e-01, 1.3112247e-02, 1.0731223e-03, ..., 3.2815991e-03,\n",
       "        3.9036016e-03, 6.9473472e-08],\n",
       "       [3.8418364e-02, 1.4409499e-03, 1.1317354e-03, ..., 5.7045547e-03,\n",
       "        4.5717005e-03, 7.2389389e-07],\n",
       "       ...,\n",
       "       [5.4133873e-02, 6.0582132e-04, 3.6296496e-04, ..., 1.7845613e-03,\n",
       "        1.0789689e-03, 9.4569430e-07],\n",
       "       [6.6219610e-03, 3.1086968e-03, 4.8757669e-05, ..., 4.1922869e-04,\n",
       "        1.9914478e-04, 1.6626602e-08],\n",
       "       [8.1586674e-02, 3.1353948e-03, 3.2573056e-03, ..., 2.8697215e-02,\n",
       "        9.8005384e-03, 1.2756771e-04]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f37ab91-699c-46ef-9496-7518d5972d63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
