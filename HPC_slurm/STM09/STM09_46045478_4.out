/vast/ac8888/MusicSpeech-STM/STM09_sklearn_classifiers.py:178: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  target.replace({
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 20 concurrent workers.
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   22.4s remaining:   33.6s
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  1.0min finished
STM_output/corpSTMnpy/BibleTTS-akuapem-twi_STMall.npy
STM_output/corpSTMnpy/BibleTTS-asante-twi_STMall.npy
STM_output/corpSTMnpy/BibleTTS-ewe_STMall.npy
STM_output/corpSTMnpy/BibleTTS-hausa_STMall.npy
STM_output/corpSTMnpy/BibleTTS-lingala_STMall.npy
STM_output/corpSTMnpy/BibleTTS-yoruba_STMall.npy
STM_output/corpSTMnpy/Buckeye_STMall.npy
STM_output/corpSTMnpy/EUROM_STMall.npy
STM_output/corpSTMnpy/HiltonMoser2022_speech_STMall.npy
STM_output/corpSTMnpy/LibriSpeech_STMall.npy
STM_output/corpSTMnpy/MediaSpeech-AR_STMall.npy
STM_output/corpSTMnpy/MediaSpeech-ES_STMall.npy
STM_output/corpSTMnpy/MediaSpeech-FR_STMall.npy
STM_output/corpSTMnpy/MediaSpeech-TR_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ab_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ar_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ba_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-be_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-bg_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-bn_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-br_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ca_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ckb_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-cnh_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-cs_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-cv_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-cy_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-da_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-de_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-dv_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-el_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-en_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-eo_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-es_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-et_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-eu_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-fa_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-fi_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-fr_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-fy-NL_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ga-IE_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-gl_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-gn_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-hi_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-hu_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-hy-AM_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-id_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ig_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-it_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ja_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ka_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-kab_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-kk_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-kmr_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ky_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-lg_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-lt_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ltg_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-lv_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-mhr_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ml_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-mn_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-mt_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-nan-tw_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-nl_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-oc_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-or_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-pl_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-pt_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ro_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ru_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-rw_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-sr_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-sv-SE_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-sw_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ta_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-th_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-tr_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-tt_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ug_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-uk_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ur_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-uz_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-vi_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-yo_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-yue_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-zh-CN_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-zh-TW_STMall.npy
STM_output/corpSTMnpy/SpeechClarity_STMall.npy
STM_output/corpSTMnpy/TAT-Vol2_STMall.npy
STM_output/corpSTMnpy/TIMIT_STMall.npy
STM_output/corpSTMnpy/TTS_Javanese_STMall.npy
STM_output/corpSTMnpy/primewords_chinese_STMall.npy
STM_output/corpSTMnpy/room_reader_STMall.npy
STM_output/corpSTMnpy/thchs30_STMall.npy
STM_output/corpSTMnpy/zeroth_korean_STMall.npy
STM_output/corpSTMnpy/Albouy2020Science_STMall.npy
STM_output/corpSTMnpy/CD_STMall.npy
STM_output/corpSTMnpy/GarlandEncyclopedia_STMall.npy
STM_output/corpSTMnpy/HiltonMoser2022_song_STMall.npy
STM_output/corpSTMnpy/IRMAS_STMall.npy
STM_output/corpSTMnpy/MTG-Jamendo_STMall.npy
STM_output/corpSTMnpy/MagnaTagATune_STMall.npy
STM_output/corpSTMnpy/NHS2_STMall.npy
STM_output/corpSTMnpy/fma_large_STMall.npy
STM_output/corpSTMnpy/ismir04_genre_STMall.npy
STM_output/corpSTMnpy/SONYC_STMall.npy
STM_output/corpSTMnpy/SONYC_augmented_STMall.npy
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
Norm: 0.20, NNZs: 2419, Bias: -2.103325, T: 852212, Avg. loss: 0.039138
Total training time: 2.83 seconds.
-- Epoch 2
Norm: 0.17, NNZs: 2419, Bias: -2.484214, T: 852212, Avg. loss: 0.182428
Total training time: 3.20 seconds.
-- Epoch 2
Norm: 0.23, NNZs: 2419, Bias: -2.159926, T: 852212, Avg. loss: 0.211693
Total training time: 3.29 seconds.
-- Epoch 2
Norm: 0.19, NNZs: 2419, Bias: -1.958751, T: 852212, Avg. loss: 0.198999
Total training time: 3.39 seconds.
-- Epoch 2
Norm: 0.17, NNZs: 2419, Bias: -0.316989, T: 852212, Avg. loss: 0.321969
Total training time: 4.12 seconds.
-- Epoch 2
Norm: 0.20, NNZs: 2419, Bias: -2.100520, T: 1704424, Avg. loss: 0.033960
Total training time: 5.66 seconds.
-- Epoch 3
Norm: 0.17, NNZs: 2419, Bias: -2.480257, T: 1704424, Avg. loss: 0.175609
Total training time: 6.41 seconds.
-- Epoch 3
Norm: 0.22, NNZs: 2419, Bias: -2.128087, T: 1704424, Avg. loss: 0.204155
Total training time: 6.58 seconds.
-- Epoch 3
Norm: 0.19, NNZs: 2419, Bias: -1.964104, T: 1704424, Avg. loss: 0.195439
Total training time: 6.78 seconds.
-- Epoch 3
Norm: 0.17, NNZs: 2419, Bias: -0.305084, T: 1704424, Avg. loss: 0.316300
Total training time: 8.23 seconds.
-- Epoch 3
Norm: 0.20, NNZs: 2419, Bias: -2.098892, T: 2556636, Avg. loss: 0.033934
Total training time: 8.49 seconds.
-- Epoch 4
Norm: 0.17, NNZs: 2419, Bias: -2.477924, T: 2556636, Avg. loss: 0.175550
Total training time: 9.61 seconds.
-- Epoch 4
Norm: 0.22, NNZs: 2419, Bias: -2.109411, T: 2556636, Avg. loss: 0.203413
Total training time: 9.88 seconds.
-- Epoch 4
Norm: 0.19, NNZs: 2419, Bias: -1.967225, T: 2556636, Avg. loss: 0.195309
Total training time: 10.18 seconds.
-- Epoch 4
Norm: 0.20, NNZs: 2419, Bias: -2.097750, T: 3408848, Avg. loss: 0.033952
Total training time: 11.33 seconds.
-- Epoch 5
Norm: 0.17, NNZs: 2419, Bias: -0.298246, T: 2556636, Avg. loss: 0.315501
Total training time: 12.35 seconds.
-- Epoch 4
Norm: 0.17, NNZs: 2419, Bias: -2.476243, T: 3408848, Avg. loss: 0.175321
Total training time: 12.82 seconds.
-- Epoch 5
Norm: 0.22, NNZs: 2419, Bias: -2.096329, T: 3408848, Avg. loss: 0.203011
Total training time: 13.18 seconds.
-- Epoch 5
Norm: 0.19, NNZs: 2419, Bias: -1.969402, T: 3408848, Avg. loss: 0.195245
Total training time: 13.57 seconds.
-- Epoch 5
Norm: 0.20, NNZs: 2419, Bias: -2.096852, T: 4261060, Avg. loss: 0.033939
Total training time: 14.16 seconds.
-- Epoch 6
Norm: 0.17, NNZs: 2419, Bias: -2.474956, T: 4261060, Avg. loss: 0.175395
Total training time: 16.02 seconds.
-- Epoch 6
Norm: 0.17, NNZs: 2419, Bias: -0.293480, T: 3408848, Avg. loss: 0.314525
Total training time: 16.46 seconds.
-- Epoch 5
Norm: 0.22, NNZs: 2419, Bias: -2.086192, T: 4261060, Avg. loss: 0.202695
Total training time: 16.47 seconds.
-- Epoch 6
Norm: 0.19, NNZs: 2419, Bias: -1.971093, T: 4261060, Avg. loss: 0.195178
Total training time: 16.96 seconds.
-- Epoch 6
Norm: 0.20, NNZs: 2419, Bias: -2.096112, T: 5113272, Avg. loss: 0.034029
Total training time: 16.99 seconds.
-- Epoch 7
Norm: 0.17, NNZs: 2419, Bias: -2.473892, T: 5113272, Avg. loss: 0.175316
Total training time: 19.23 seconds.
-- Epoch 7
Norm: 0.22, NNZs: 2419, Bias: -2.077920, T: 5113272, Avg. loss: 0.202449
Total training time: 19.77 seconds.
-- Epoch 7
Norm: 0.20, NNZs: 2419, Bias: -2.095521, T: 5965484, Avg. loss: 0.033784
Total training time: 19.82 seconds.
Convergence after 7 epochs took 19.82 seconds
Norm: 0.19, NNZs: 2419, Bias: -1.972487, T: 5113272, Avg. loss: 0.195281
Total training time: 20.35 seconds.
-- Epoch 7
Norm: 0.17, NNZs: 2419, Bias: -0.289823, T: 4261060, Avg. loss: 0.314033
Total training time: 20.56 seconds.
-- Epoch 6
Norm: 0.17, NNZs: 2419, Bias: -2.473010, T: 5965484, Avg. loss: 0.175301
Total training time: 22.42 seconds.
Convergence after 7 epochs took 22.42 seconds
Norm: 0.22, NNZs: 2419, Bias: -2.071012, T: 5965484, Avg. loss: 0.202230
Total training time: 23.06 seconds.
Convergence after 7 epochs took 23.06 seconds
Norm: 0.19, NNZs: 2419, Bias: -1.973650, T: 5965484, Avg. loss: 0.195065
Total training time: 23.72 seconds.
Convergence after 7 epochs took 23.72 seconds
Norm: 0.17, NNZs: 2419, Bias: -0.286832, T: 5113272, Avg. loss: 0.314134
Total training time: 24.64 seconds.
-- Epoch 7
Norm: 0.17, NNZs: 2419, Bias: -0.284368, T: 5965484, Avg. loss: 0.312903
Total training time: 28.69 seconds.
-- Epoch 8
Norm: 0.16, NNZs: 2419, Bias: -0.282172, T: 6817696, Avg. loss: 0.314257
Total training time: 32.74 seconds.
-- Epoch 9
Norm: 0.17, NNZs: 2419, Bias: -0.280307, T: 7669908, Avg. loss: 0.312734
Total training time: 36.78 seconds.
-- Epoch 10
Norm: 0.17, NNZs: 2419, Bias: -0.278674, T: 8522120, Avg. loss: 0.311567
Total training time: 40.83 seconds.
-- Epoch 11
Norm: 0.16, NNZs: 2419, Bias: -0.277180, T: 9374332, Avg. loss: 0.312059
Total training time: 44.87 seconds.
-- Epoch 12
Norm: 0.16, NNZs: 2419, Bias: -0.275773, T: 10226544, Avg. loss: 0.313592
Total training time: 48.93 seconds.
-- Epoch 13
Norm: 0.16, NNZs: 2419, Bias: -0.274423, T: 11078756, Avg. loss: 0.315922
Total training time: 52.99 seconds.
-- Epoch 14
Norm: 0.16, NNZs: 2419, Bias: -0.273195, T: 11930968, Avg. loss: 0.315316
Total training time: 57.04 seconds.
-- Epoch 15
Norm: 0.17, NNZs: 2419, Bias: -0.272135, T: 12783180, Avg. loss: 0.311583
Total training time: 61.08 seconds.
Convergence after 15 epochs took 61.08 seconds
Traceback (most recent call last):
  File "/vast/ac8888/MusicSpeech-STM/STM09_sklearn_classifiers.py", line 623, in <module>
    run_SGDClinearSVC(X_train, X_val, X_test, y_train, y_val, y_test)
  File "/vast/ac8888/MusicSpeech-STM/STM09_sklearn_classifiers.py", line 239, in run_SGDClinearSVC
    bo_SGDClinearSVC.maximize(n_iter=200, init_points=25)
  File "/ext3/miniconda3/envs/MusicSpeech-STMhpc/lib/python3.11/site-packages/bayes_opt/bayesian_optimization.py", line 310, in maximize
    self.probe(x_probe, lazy=False)
  File "/ext3/miniconda3/envs/MusicSpeech-STMhpc/lib/python3.11/site-packages/bayes_opt/bayesian_optimization.py", line 208, in probe
    self._space.probe(params)
  File "/ext3/miniconda3/envs/MusicSpeech-STMhpc/lib/python3.11/site-packages/bayes_opt/target_space.py", line 236, in probe
    target = self.target_func(**params)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/ac8888/MusicSpeech-STM/STM09_sklearn_classifiers.py", line 224, in bo_tune_SGDClinearSVC
    y_val_encoded = OneHotEncoder(sparse_output=False).fit_transform(y_val)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/MusicSpeech-STMhpc/lib/python3.11/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/MusicSpeech-STMhpc/lib/python3.11/site-packages/sklearn/base.py", line 1098, in fit_transform
    return self.fit(X, **fit_params).transform(X)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/MusicSpeech-STMhpc/lib/python3.11/site-packages/sklearn/base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/MusicSpeech-STMhpc/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py", line 975, in fit
    self._fit(
  File "/ext3/miniconda3/envs/MusicSpeech-STMhpc/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py", line 78, in _fit
    X_list, n_samples, n_features = self._check_X(
                                    ^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/MusicSpeech-STMhpc/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py", line 44, in _check_X
    X_temp = check_array(X, dtype=None, force_all_finite=force_all_finite)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/MusicSpeech-STMhpc/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1035, in check_array
    raise ValueError(msg)
ValueError: Expected a 2-dimensional container but got <class 'pandas.core.series.Series'> instead. Pass a DataFrame containing a single row (i.e. single sample) or a single column (i.e. single feature) instead.
