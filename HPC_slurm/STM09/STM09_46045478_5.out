/vast/ac8888/MusicSpeech-STM/STM09_sklearn_classifiers.py:178: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  target.replace({
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 20 concurrent workers.
[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  1.8min remaining:  2.7min
[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.1min finished
STM_output/corpSTMnpy/BibleTTS-akuapem-twi_STMall.npy
STM_output/corpSTMnpy/BibleTTS-asante-twi_STMall.npy
STM_output/corpSTMnpy/BibleTTS-ewe_STMall.npy
STM_output/corpSTMnpy/BibleTTS-hausa_STMall.npy
STM_output/corpSTMnpy/BibleTTS-lingala_STMall.npy
STM_output/corpSTMnpy/BibleTTS-yoruba_STMall.npy
STM_output/corpSTMnpy/Buckeye_STMall.npy
STM_output/corpSTMnpy/EUROM_STMall.npy
STM_output/corpSTMnpy/HiltonMoser2022_speech_STMall.npy
STM_output/corpSTMnpy/LibriSpeech_STMall.npy
STM_output/corpSTMnpy/MediaSpeech-AR_STMall.npy
STM_output/corpSTMnpy/MediaSpeech-ES_STMall.npy
STM_output/corpSTMnpy/MediaSpeech-FR_STMall.npy
STM_output/corpSTMnpy/MediaSpeech-TR_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ab_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ar_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ba_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-be_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-bg_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-bn_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-br_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ca_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ckb_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-cnh_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-cs_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-cv_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-cy_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-da_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-de_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-dv_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-el_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-en_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-eo_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-es_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-et_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-eu_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-fa_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-fi_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-fr_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-fy-NL_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ga-IE_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-gl_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-gn_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-hi_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-hu_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-hy-AM_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-id_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ig_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-it_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ja_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ka_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-kab_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-kk_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-kmr_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ky_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-lg_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-lt_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ltg_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-lv_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-mhr_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ml_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-mn_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-mt_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-nan-tw_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-nl_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-oc_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-or_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-pl_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-pt_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ro_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ru_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-rw_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-sr_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-sv-SE_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-sw_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ta_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-th_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-tr_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-tt_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ug_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-uk_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-ur_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-uz_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-vi_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-yo_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-yue_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-zh-CN_STMall.npy
STM_output/corpSTMnpy/MozillaCommonVoice-zh-TW_STMall.npy
STM_output/corpSTMnpy/SpeechClarity_STMall.npy
STM_output/corpSTMnpy/TAT-Vol2_STMall.npy
STM_output/corpSTMnpy/TIMIT_STMall.npy
STM_output/corpSTMnpy/TTS_Javanese_STMall.npy
STM_output/corpSTMnpy/primewords_chinese_STMall.npy
STM_output/corpSTMnpy/room_reader_STMall.npy
STM_output/corpSTMnpy/thchs30_STMall.npy
STM_output/corpSTMnpy/zeroth_korean_STMall.npy
STM_output/corpSTMnpy/Albouy2020Science_STMall.npy
STM_output/corpSTMnpy/CD_STMall.npy
STM_output/corpSTMnpy/GarlandEncyclopedia_STMall.npy
STM_output/corpSTMnpy/HiltonMoser2022_song_STMall.npy
STM_output/corpSTMnpy/IRMAS_STMall.npy
STM_output/corpSTMnpy/MTG-Jamendo_STMall.npy
STM_output/corpSTMnpy/MagnaTagATune_STMall.npy
STM_output/corpSTMnpy/NHS2_STMall.npy
STM_output/corpSTMnpy/fma_large_STMall.npy
STM_output/corpSTMnpy/ismir04_genre_STMall.npy
STM_output/corpSTMnpy/SONYC_STMall.npy
STM_output/corpSTMnpy/SONYC_augmented_STMall.npy
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
-- Epoch 1
Norm: 7.39, NNZs: 0, Bias: -0.489135, T: 852212, Avg. loss: 0.764335
Total training time: 15.27 seconds.
-- Epoch 2
Norm: 16.16, NNZs: 0, Bias: -1.321532, T: 852212, Avg. loss: 0.406497
Total training time: 15.40 seconds.
-- Epoch 2
Norm: 14.04, NNZs: 0, Bias: -1.320008, T: 852212, Avg. loss: 0.335960
Total training time: 15.48 seconds.
-- Epoch 2
Norm: 12.34, NNZs: 0, Bias: -1.312897, T: 852212, Avg. loss: 0.364098
Total training time: 15.54 seconds.
-- Epoch 2
Norm: 13.05, NNZs: 0, Bias: -1.253356, T: 852212, Avg. loss: 0.395969
Total training time: 15.58 seconds.
-- Epoch 2
Norm: 7.02, NNZs: 0, Bias: -0.501799, T: 1704424, Avg. loss: 0.770177
Total training time: 30.54 seconds.
-- Epoch 3
Norm: 15.36, NNZs: 0, Bias: -1.332211, T: 1704424, Avg. loss: 0.404450
Total training time: 30.80 seconds.
-- Epoch 3
Norm: 13.34, NNZs: 0, Bias: -1.335590, T: 1704424, Avg. loss: 0.330933
Total training time: 30.97 seconds.
-- Epoch 3
Norm: 11.73, NNZs: 0, Bias: -1.327089, T: 1704424, Avg. loss: 0.360179
Total training time: 31.08 seconds.
-- Epoch 3
Norm: 12.41, NNZs: 0, Bias: -1.270507, T: 1704424, Avg. loss: 0.391562
Total training time: 31.16 seconds.
-- Epoch 3
Norm: 6.99, NNZs: 0, Bias: -0.508869, T: 2556636, Avg. loss: 0.772206
Total training time: 45.81 seconds.
-- Epoch 4
Norm: 15.29, NNZs: 0, Bias: -1.338084, T: 2556636, Avg. loss: 0.403814
Total training time: 46.20 seconds.
-- Epoch 4
Norm: 13.28, NNZs: 0, Bias: -1.344150, T: 2556636, Avg. loss: 0.329378
Total training time: 46.45 seconds.
-- Epoch 4
Norm: 11.68, NNZs: 0, Bias: -1.334952, T: 2556636, Avg. loss: 0.358948
Total training time: 46.63 seconds.
-- Epoch 4
Norm: 12.35, NNZs: 0, Bias: -1.279914, T: 2556636, Avg. loss: 0.390227
Total training time: 46.73 seconds.
-- Epoch 4
Norm: 6.99, NNZs: 0, Bias: -0.513722, T: 3408848, Avg. loss: 0.773467
Total training time: 61.09 seconds.
-- Epoch 5
Norm: 15.29, NNZs: 0, Bias: -1.342100, T: 3408848, Avg. loss: 0.403425
Total training time: 61.59 seconds.
-- Epoch 5
Norm: 13.28, NNZs: 0, Bias: -1.349988, T: 3408848, Avg. loss: 0.328429
Total training time: 61.93 seconds.
-- Epoch 5
Norm: 11.68, NNZs: 0, Bias: -1.340300, T: 3408848, Avg. loss: 0.358213
Total training time: 62.18 seconds.
-- Epoch 5
Norm: 12.35, NNZs: 0, Bias: -1.286334, T: 3408848, Avg. loss: 0.389414
Total training time: 62.29 seconds.
-- Epoch 5
Norm: 6.99, NNZs: 0, Bias: -0.517411, T: 4261060, Avg. loss: 0.774403
Total training time: 76.36 seconds.
-- Epoch 6
Norm: 15.29, NNZs: 0, Bias: -1.345131, T: 4261060, Avg. loss: 0.403147
Total training time: 77.00 seconds.
-- Epoch 6
Norm: 13.28, NNZs: 0, Bias: -1.354389, T: 4261060, Avg. loss: 0.327752
Total training time: 77.41 seconds.
-- Epoch 6
Norm: 11.68, NNZs: 0, Bias: -1.344336, T: 4261060, Avg. loss: 0.357683
Total training time: 77.72 seconds.
-- Epoch 6
Norm: 12.35, NNZs: 0, Bias: -1.291159, T: 4261060, Avg. loss: 0.388843
Total training time: 77.88 seconds.
-- Epoch 6
Norm: 6.99, NNZs: 0, Bias: -0.520369, T: 5113272, Avg. loss: 0.775125
Total training time: 91.64 seconds.
Convergence after 6 epochs took 91.64 seconds
Norm: 15.29, NNZs: 0, Bias: -1.347560, T: 5113272, Avg. loss: 0.402930
Total training time: 92.39 seconds.
-- Epoch 7
Norm: 13.28, NNZs: 0, Bias: -1.357898, T: 5113272, Avg. loss: 0.327234
Total training time: 92.95 seconds.
-- Epoch 7
Norm: 11.68, NNZs: 0, Bias: -1.347559, T: 5113272, Avg. loss: 0.357275
Total training time: 93.35 seconds.
-- Epoch 7
Norm: 12.35, NNZs: 0, Bias: -1.295014, T: 5113272, Avg. loss: 0.388399
Total training time: 93.47 seconds.
-- Epoch 7
Norm: 13.28, NNZs: 0, Bias: -1.360808, T: 5965484, Avg. loss: 0.326812
Total training time: 108.41 seconds.
-- Epoch 8
Norm: 11.68, NNZs: 0, Bias: -1.350235, T: 5965484, Avg. loss: 0.356943
Total training time: 108.75 seconds.
-- Epoch 8
Norm: 12.35, NNZs: 0, Bias: -1.298210, T: 5965484, Avg. loss: 0.388042
Total training time: 109.05 seconds.
-- Epoch 8
Norm: 15.29, NNZs: 0, Bias: -1.349571, T: 5965484, Avg. loss: 0.402758
Total training time: 109.07 seconds.
Convergence after 7 epochs took 109.07 seconds
Norm: 13.28, NNZs: 0, Bias: -1.363289, T: 6817696, Avg. loss: 0.326458
Total training time: 123.88 seconds.
Convergence after 8 epochs took 123.88 seconds
Norm: 11.68, NNZs: 0, Bias: -1.352514, T: 6817696, Avg. loss: 0.356668
Total training time: 124.16 seconds.
Convergence after 8 epochs took 124.16 seconds
Norm: 12.35, NNZs: 0, Bias: -1.300935, T: 6817696, Avg. loss: 0.387741
Total training time: 125.62 seconds.
Convergence after 8 epochs took 125.62 seconds
Traceback (most recent call last):
  File "/vast/ac8888/MusicSpeech-STM/STM09_sklearn_classifiers.py", line 625, in <module>
    run_SGDClogReg(X_train, X_val, X_test, y_train, y_val, y_test)
  File "/vast/ac8888/MusicSpeech-STM/STM09_sklearn_classifiers.py", line 351, in run_SGDClogReg
    bo_SGDClogReg.maximize(n_iter=200, init_points=25)
  File "/ext3/miniconda3/envs/MusicSpeech-STMhpc/lib/python3.11/site-packages/bayes_opt/bayesian_optimization.py", line 310, in maximize
    self.probe(x_probe, lazy=False)
  File "/ext3/miniconda3/envs/MusicSpeech-STMhpc/lib/python3.11/site-packages/bayes_opt/bayesian_optimization.py", line 208, in probe
    self._space.probe(params)
  File "/ext3/miniconda3/envs/MusicSpeech-STMhpc/lib/python3.11/site-packages/bayes_opt/target_space.py", line 236, in probe
    target = self.target_func(**params)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vast/ac8888/MusicSpeech-STM/STM09_sklearn_classifiers.py", line 335, in bo_tune_SGDClogReg
    y_val_encoded = OneHotEncoder(sparse_output=False).fit_transform(y_val)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/MusicSpeech-STMhpc/lib/python3.11/site-packages/sklearn/utils/_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/MusicSpeech-STMhpc/lib/python3.11/site-packages/sklearn/base.py", line 1098, in fit_transform
    return self.fit(X, **fit_params).transform(X)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/MusicSpeech-STMhpc/lib/python3.11/site-packages/sklearn/base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/MusicSpeech-STMhpc/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py", line 975, in fit
    self._fit(
  File "/ext3/miniconda3/envs/MusicSpeech-STMhpc/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py", line 78, in _fit
    X_list, n_samples, n_features = self._check_X(
                                    ^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/MusicSpeech-STMhpc/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py", line 44, in _check_X
    X_temp = check_array(X, dtype=None, force_all_finite=force_all_finite)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ext3/miniconda3/envs/MusicSpeech-STMhpc/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1035, in check_array
    raise ValueError(msg)
ValueError: Expected a 2-dimensional container but got <class 'pandas.core.series.Series'> instead. Pass a DataFrame containing a single row (i.e. single sample) or a single column (i.e. single feature) instead.
